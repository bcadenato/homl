---
title: "Linear Regression"
output: html_document
date: "2023-09-17"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Regression

We'll load the tidyverse package, but we'll explicitly name other packages to reinforce learning on their functions and package source.

```{r echo=TRUE}
# Helper packages
library(tidyverse)
```

We'll use data from the previous chapter.

```{r}
ames_train <- read_rds("results/ch01_ames_train.rds")
ames_test <- read_rds("results/ch01_ames_test.rds")
```

### Simple linear regression

#### Estimation

The traditional specification of this model in R is done with the `lm` function.

```{r}
model1 <- lm(
    Sale_Price ~ Gr_Liv_Area,
    data = ames_train)
```

This function does several things, that in the `tidymodels` would be expected to be done in different steps: 
 - It specifies the model
 - It fits the model
 
 The function `summary` is a generic that knows how to interpret the model and display it for a user.
 
```{r}
summary(model1)
```

And if we are only interested in the coefficients the function `coef` can provide these:

```{r}
coef(model1)
```

If we wanted to replicate this modelling with the `tidymodels` package instead, it could be done with the following steps:

```{r}
model_wf_01 <- workflows::workflow() %>% 
    workflows::add_formula(
        Sale_Price ~ Gr_Liv_Area) %>% 
    workflows::add_model(
        parsnip::linear_reg() %>% 
            parsnip::set_engine("lm") %>% 
            parsnip::set_mode("regression")) %>% 
    parsnip::fit(
        data = ames_train)
```

The `broom` package provides functions to inspect the model parameters in a way that can be easily manipulated later:

```{r}
broom::tidy(model_wf_01)
```

#### Inference
 
The function `confint` provides an interval for the parameters for a given level of confidence:

```{r}
confint(model1, level = 0.95)
```

### Multiple linear regression

A multiple linear regression can be also run with the same model `lm`.

```{r}
model2 <- lm(
    Sale_Price ~ Gr_Liv_Area + Year_Built,
    data = ames_train)
```

With `tidymodels` this would be done with:

```{r}
model_wf_02 <- model_wf_01 %>% 
    workflows::update_formula(
        Sale_Price ~ Gr_Liv_Area + Year_Built) %>% 
    parsnip::fit(
        data = ames_train)
```

Interactions can be included in the model as well:

```{r}
lm(
    Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built,
    data = ames_train)
```

Finally the model could have been run over the whole set of available features.

```{r}
model3 <- lm(
    Sale_Price ~ .,
    data = ames_train)

broom::tidy(model3)
```

```{r}
model_wf_03 <- model_wf_02 %>% 
    workflows::update_formula(
        Sale_Price ~ .) %>% 
    parsnip::fit(
        data = ames_train)
```

Notice how there's been some feature engineering behind the scenes, as this is required by the `lm` model. The problem is that we don't control in this case explicitly what's been done.

### Assessing model's performance

To define model's performance it's important to define what we mean by "better". Usual measures are RMSE (root mean squared error) o MAE (mean absolute error). These are measures of how well the model can predict the target given some features for an observation that the model hasn't seen before.

In practice there are other relevant measures such as the time required to train the model, make a prediction or the computational effort required.

As discussed elsewhere it's better to train the model with an analysis set and measure performance with an assessment set, where both the analysis and assessment sets are subsets of the training set. Then the testing set is only used to measure effectiveness of the final selected model. 

As seen in the book so far, this can be done either with the `caret` package or with the more modern `tidymodels` set of packages.

```{r}
# Train model with 10-fold cross validation using caret

set.seed(123)

cv_model_01 <- caret::train(
    form = Sale_Price ~ Gr_Liv_Area,
    data = ames_train,
    method = "lm",
    trControl = caret::trainControl(
        method = "cv",
        number = 10))
```

Let's do the same using the `tidymodels` packages instead.

```{r}
# V-Fold cross-validation

vfold_01 <- rsample::vfold_cv(
    data = ames_train,
    v = 10)

# Fit resamples and measure performance 

cv_model_02 <- tune::fit_resamples(
    object = model_wf_01,
    resamples = vfold_01,
    metrics = yardstick::metric_set(
        yardstick::rmse,
        yardstick::mae),
    control = tune::control_resamples(
        save_pred = TRUE)
    )

tune::collect_metrics(cv_model_02)

tune::collect_predictions(cv_model_02)
```

Same model cross validation for the other two models.

```{r}
cv_model_11 <- caret::train(
    form = Sale_Price ~ Gr_Liv_Area + Year_Built,
    data = ames_train,
    method = "lm",
    trControl = caret::trainControl(
        method = "cv",
        number = 10))

cv_model_12 <- tune::fit_resamples(
    object = model_wf_02,
    resamples = vfold_01,
    metrics = yardstick::metric_set(
        yardstick::rmse,
        yardstick::mae),
    control = tune::control_resamples(
        save_pred = TRUE)
    )

tune::collect_metrics(cv_model_12)

tune::collect_predictions(cv_model_02)
```

```{r}
cv_model_21 <- caret::train(
    form = Sale_Price ~ .,
    data = ames_train,
    method = "lm",
    trControl = caret::trainControl(
        method = "cv",
        number = 10))
        
cv_model_22 <- tune::fit_resamples(
    object = model_wf_03,
    resamples = vfold_01,
    metrics = yardstick::metric_set(
        yardstick::rmse,
        yardstick::mae),
    control = tune::control_resamples(
        save_pred = TRUE)
    )

tune::collect_metrics(
    cv_model_22)

tune::collect_predictions(
    cv_model_22)
```

```{r}
summary(
    caret::resamples(
        list(
            model1 = cv_model_01,
            model2 = cv_model_11,
            model3 = cv_model_21)))
```

### Model concerns

When using linear regression models there are many ways in which the data departs from model assumptions.

#### Linear relationship

```{r}
p01 <- ggplot(
    data = ames_train,
    mapping = aes(
        x = Year_Built,
        y = Sale_Price)) +
    geom_point(
        size = 1,
        alpha = 0.4) +
    geom_smooth(
        method = "lm",
        se = TRUE) +
    scale_y_continuous(
        "Sale Price",
        labels = scales::dollar) +
    xlab("Year built") +
    ggtitle("Non-transformed variables with non-linear relationships")

p02 <- ggplot(
    data = ames_train,
    mapping = aes(
        x = Year_Built,
        y = Sale_Price)) +
    geom_point(
        size = 1,
        alpha = 0.4) +
    geom_smooth(
        method = "lm",
        se = TRUE) +
    scale_y_log10(
        "Sale Price",
        labels = scales::dollar,
        breaks = seq(0, 400000, by = 100000)) +
    xlab("Year built") +
    ggtitle("Transforming variables can provide a near-linear relationship")

gridExtra::grid.arrange(
    p01, 
    p02,
    nrow = 1)
```

#### Constant variance amongst residuals

A linear regression model assumes residuals have constant variance across observations. The following charts show how the assumption is violated in the simple regression model with only one variable.

```{r}
df_01 <- broom::augment(
    cv_model_01$finalModel,
    data = ames_train)

p11 <- ggplot(
    data = df_01,
    mapping = aes(
        x = .fitted,
        y = .resid))

df_02 <- broom::augment(
    cv_model_02)

p12 <- ggplot(
    data = df_02,
    mapping = aes(
        x = .pred,
        y = .resid)) +
    geom_point(
        size = 1,
        alpha = 4) +
    xlab("Predictions") +
    scale_x_continuous(
        labels = scales::label_comma()) +
    ylab("Residuals") +
    scale_y_continuous(
        labels = scales::label_comma()) +
    ggtitle(
        "Model 1",
        subtitle = "Sale_Price ~ Gr_Liv_Area")

df_22 <- broom::augment(
    cv_model_22)

p14 <- ggplot(
    data = df_22,
    mapping = aes(
        x = .pred,
        y = .resid)) +
    geom_point(
        size = 1,
        alpha = 4) +
    xlab("Predictions") +
    scale_x_continuous(
        labels = scales::label_comma()) +
    ylab("Residuals") +
    scale_y_continuous(
        labels = scales::label_comma()) + 
    ggtitle(
        "Model 3",
        subtitle = "Sale_Price ~ .")

gridExtra::grid.arrange(
    p12,
    p14,
    nrow = 1)
```

#### No autocorrelation

Linear regression models assume that residuals are independent and uncorrelated. If this is not true, then the standard errors of the coefficients will be biased, leading to narrower prediction intervals that they should.

#### More observations than predictors

The OLS are not obtainable if the number of predictors is larger than the number of observations.

#### Multicollinearity

The presence of collinearity can pose problems in the OLS, since it can be difficult to separate out the individual effects of collinear variables on the response.

### Principal Component Regression

Principal component analysis is a technique to reduce a set of many variables that might be highly correlated down to a smaller number of metrics that are independent, and capture as much variability as possible.

This step can be done with the `tidymodels` `recipes` package. Previous normalization of the variables is advised.

The book shows an example with `caret`, but I'll show how to do the same with `tidymodels` instead.

```{r}
pcr_rec_01 <- recipes::recipe(
    Sale_Price ~ .,
    data = ames_train) %>% 
    recipes::step_novel(
        recipes::all_factor_predictors()) %>% 
    recipes::step_dummy(
        recipes::all_factor_predictors()) %>% 
    recipes::step_nzv(
        recipes::all_predictors()) %>% 
    recipes::step_normalize(
        recipes::all_numeric_predictors()) %>% 
    recipes::step_pca(
        recipes::all_numeric_predictors(),
        num_comp = 5)

pcr_model_01 <- parsnip::linear_reg() %>% 
    parsnip::set_engine("lm") %>% 
    parsnip::set_mode("regression")

pcr_folds_01 <- rsample::vfold_cv(
    data = ames_train,
    v = 10)

pcr_wf_01 <- workflows::workflow() %>% 
    workflows::add_recipe(pcr_rec_01) %>% 
    workflows::add_model(
        pcr_model_01)

# Fit resamples and measure performance of the model
        
pcr_wf_02 <- pcr_wf_01 %>% 
    tune::fit_resamples(
        pcr_folds_01,
        control = tune::control_resamples(
            verbose = FALSE))

pcr_wf_02 %>% 
    tune::collect_metrics()

# Final fit with all the training data

pcr_wf_03 <- pcr_wf_01 %>% 
    parsnip::fit(
        data = ames_train)

pcr_wf_03 %>% 
    tidy()

```

### Partial least squares

This is a similar technique to principal component analysis, but in these case the principal components are calculated so as to maximise correlation to the outcome variable.

### Feature interpretation

The interpretation of features in linear regression models is straight-forward. The model assumes a monotonic linear relationship between the predictor variables and the response.

The `vip` package allows to visualize the most important variables in a linear regresion model.

```{r}
vip::vip(model_wf_03)
```






























